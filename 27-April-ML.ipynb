{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7441b116-657e-4680-b373-cf7cb0a76c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe main types of clustering in unsupervised machine learning include K-means, hierarchical clustering,\\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN), and Gaussian Mixtures Model (GMM).\\nK-Means clustering method considers two assumptions regarding the clusters – first that the clusters \\nare spherical and second that the clusters are of similar size. \\nSpherical assumption helps in separating the clusters when the algorithm works on the data and forms clusters.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 1 :\n",
    "\"\"\"\n",
    "The main types of clustering in unsupervised machine learning include K-means, hierarchical clustering,\n",
    "Density-Based Spatial Clustering of Applications with Noise (DBSCAN), and Gaussian Mixtures Model (GMM).\n",
    "K-Means clustering method considers two assumptions regarding the clusters – first that the clusters \n",
    "are spherical and second that the clusters are of similar size. \n",
    "Spherical assumption helps in separating the clusters when the algorithm works on the data and forms clusters.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "835cb602-6f37-4657-b3fc-3d8cede49f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nK-means is a centroid-based clustering algorithm, where we calculate the distance between each data point and a\\ncentroid to assign it to a cluster. The goal is to identify the K number of groups in the dataset.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 2 :\n",
    "\"\"\"\n",
    "K-means is a centroid-based clustering algorithm, where we calculate the distance between each data point and a\n",
    "centroid to assign it to a cluster. The goal is to identify the K number of groups in the dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c130b176-1e9f-4724-9710-9a8feaf2f955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdvantages of k-means\\n1.Relatively simple to implement.\\n\\n2.Scales to large data sets.\\n\\n3.Guarantees convergence.\\n\\n4.Can warm-start the positions of centroids.\\n\\n5.Easily adapts to new examples.\\n\\n6.Generalizes to clusters of different shapes and sizes, such as elliptical clusters.\\n\\nDisadvantages of k-means\\n1.Choosing k manually.\\n\\nUse the “Loss vs. Clusters” plot to find the optimal (k), as discussed in Interpret Results.\\n\\n2.Being dependent on initial values.\\n\\nFor a low , you can mitigate this dependence by running k-means several times with different initial values and picking\\nthe best result.As k increases, you need advanced versions of k-means to pick better values of the initial centroids \\n(called k-means seeding).\\nFor a full discussion of k- means seeding see, A Comparative Study of Efficient Initialization Methods \\nfor the K-Means Clustering Algorithm by M. Emre Celebi, Hassan A. Kingravi, Patricio A. Vela.\\n\\n3.Clustering data of varying sizes and density.\\n\\nk-means has trouble clustering data where clusters are of varying sizes and density.\\nTo cluster such data, you need to generalize k-means as described in the Advantages section.\\n\\n4.Clustering outliers.\\n\\nCentroids can be dragged by outliers, or outliers might get their own cluster instead of being ignored. \\nConsider removing or clipping outliers before clustering.\\n\\n5.Scaling with number of dimensions.\\n\\nAs the number of dimensions increases, a distance-based similarity measure converges to a constant value between\\nany given examples.\\nReduce dimensionality either by using PCA on the feature data, or by using “spectral clustering” to modify \\nthe clustering algorithm as explained below.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 3 :\n",
    "\"\"\"\n",
    "Advantages of k-means\n",
    "1.Relatively simple to implement.\n",
    "\n",
    "2.Scales to large data sets.\n",
    "\n",
    "3.Guarantees convergence.\n",
    "\n",
    "4.Can warm-start the positions of centroids.\n",
    "\n",
    "5.Easily adapts to new examples.\n",
    "\n",
    "6.Generalizes to clusters of different shapes and sizes, such as elliptical clusters.\n",
    "\n",
    "Disadvantages of k-means\n",
    "1.Choosing k manually.\n",
    "\n",
    "Use the “Loss vs. Clusters” plot to find the optimal (k), as discussed in Interpret Results.\n",
    "\n",
    "2.Being dependent on initial values.\n",
    "\n",
    "For a low , you can mitigate this dependence by running k-means several times with different initial values and picking\n",
    "the best result.As k increases, you need advanced versions of k-means to pick better values of the initial centroids \n",
    "(called k-means seeding).\n",
    "For a full discussion of k- means seeding see, A Comparative Study of Efficient Initialization Methods \n",
    "for the K-Means Clustering Algorithm by M. Emre Celebi, Hassan A. Kingravi, Patricio A. Vela.\n",
    "\n",
    "3.Clustering data of varying sizes and density.\n",
    "\n",
    "k-means has trouble clustering data where clusters are of varying sizes and density.\n",
    "To cluster such data, you need to generalize k-means as described in the Advantages section.\n",
    "\n",
    "4.Clustering outliers.\n",
    "\n",
    "Centroids can be dragged by outliers, or outliers might get their own cluster instead of being ignored. \n",
    "Consider removing or clipping outliers before clustering.\n",
    "\n",
    "5.Scaling with number of dimensions.\n",
    "\n",
    "As the number of dimensions increases, a distance-based similarity measure converges to a constant value between\n",
    "any given examples.\n",
    "Reduce dimensionality either by using PCA on the feature data, or by using “spectral clustering” to modify \n",
    "the clustering algorithm as explained below.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ceb7f5d-34cb-438e-8316-2b3bcb21515c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nk-Means Elbow method :\\nThe k-Means Elbow method is used to find the optimal value of the K in the K-Means algorithm.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 4 :\n",
    "\"\"\"\n",
    "k-Means Elbow method :\n",
    "The k-Means Elbow method is used to find the optimal value of the K in the K-Means algorithm.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6c743ed-524b-4f13-8eb9-905a119ab51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nApplications of K-means clustering:\\nK-means clustering can be used in almost every domain, ranging from banking to recommendation engines,\\ncyber security, document clustering to image segmentation.\\nIt is typically applied to data that has a smaller number of dimensions, is numeric, and is continuous.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 5 :\n",
    "\"\"\"\n",
    "Applications of K-means clustering:\n",
    "K-means clustering can be used in almost every domain, ranging from banking to recommendation engines,\n",
    "cyber security, document clustering to image segmentation.\n",
    "It is typically applied to data that has a smaller number of dimensions, is numeric, and is continuous.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71201eed-3055-495a-900f-988ede79d87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nInterpreting the meaning of k-means clusters boils down to characterizing the clusters.\\nA Parallel Coordinates Plot allows us to see how individual data points sit across all variables. \\nBy looking at how the values for each variable compare across clusters, we can get a sense of what each cluster represents.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  Q No. 6 :\n",
    "\"\"\"\n",
    "Interpreting the meaning of k-means clusters boils down to characterizing the clusters.\n",
    "A Parallel Coordinates Plot allows us to see how individual data points sit across all variables. \n",
    "By looking at how the values for each variable compare across clusters, we can get a sense of what each cluster represents.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef00fe99-6fd2-441f-9ed2-d1759a771c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nk-Means doesn't perform well if the clusters have varying sizes, different densities, or non-spherical shapes.\\nHas to be run for a certain amount of iteration or it would produce a suboptimal result. \\nComputationally expensive as distance is to be calculated from each centroid to all data points.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 7 :\n",
    "\"\"\"\n",
    "k-Means doesn't perform well if the clusters have varying sizes, different densities, or non-spherical shapes.\n",
    "Has to be run for a certain amount of iteration or it would produce a suboptimal result. \n",
    "Computationally expensive as distance is to be calculated from each centroid to all data points.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
